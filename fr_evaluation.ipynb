{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2 \n",
    "import socket\n",
    "import struct\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine \n",
    "from scipy.spatial import distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tshark_directory = os.path.join('.', 'traffictracer')\n",
    "statistic_directory = ['WLAN_statistics', 'Meta_statistics'] \n",
    "ori_directory = ['WLAN', 'Meta'] \n",
    "port_directory = 'tshark_port' \n",
    "conn_directory = 'conn_in_out' \n",
    "evaluation_directory = 'evaluation'\n",
    "data_date = '24-11-07'\n",
    "\n",
    "meta_prefix = 'Meta-' \n",
    "wlan_prefix = 'WLAN-' \n",
    "conn_prefix = 'conn-in-out-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sni = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'sni', data_date + '.csv')) \n",
    "new_sni_dict = {\n",
    "    'Name': [], \n",
    "    'SNI': []\n",
    "} \n",
    "new_sni_dict['Name'] = df_sni['Name'] \n",
    "new_sni_dict['SNI'] = df_sni['FilteredSNI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connSni(): \n",
    "    for conn_info in os.listdir(os.path.join(tshark_directory, conn_directory)): \n",
    "        if conn_info.startswith(conn_prefix + data_date): \n",
    "            df_conn = pd.read_csv(os.path.join(tshark_directory, conn_directory, conn_info)) \n",
    "            file_name = conn_info[12:-4]\n",
    "            name_list = new_sni_dict['Name'].tolist()\n",
    "            index = name_list.index(file_name) \n",
    "            sni_set = eval(new_sni_dict['SNI'][index]) \n",
    "            df_conn_sni = df_conn[df_conn['Server Name'].isin(sni_set)] \n",
    "            df_conn_sni.to_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flowCounts(): \n",
    "    for conn_sni in os.listdir(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni')): \n",
    "        if conn_sni.startswith(conn_prefix + data_date): \n",
    "            df_conn_sni = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_sni)) \n",
    "            flow_counts = df_conn_sni['W Flows'].value_counts().sort_index()\n",
    "            df_flow_counts = flow_counts.to_frame()\n",
    "            df_flow_counts.to_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'flow_counts', conn_sni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flowCounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_dict = {\n",
    "    'OriConn': [], \n",
    "    'FilteredConn': []\n",
    "}\n",
    "def lengthComparison(): \n",
    "    for conn_info in os.listdir(os.path.join(tshark_directory, conn_directory)): \n",
    "        if conn_info.startswith(conn_prefix + data_date): \n",
    "            df_conn = pd.read_csv(os.path.join(tshark_directory, conn_directory, conn_info)) \n",
    "            df_conn_sni = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_info)) \n",
    "            length_dict['OriConn'].append(df_conn.shape[0]) \n",
    "            length_dict['FilteredConn'].append(df_conn_sni.shape[0]) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengthComparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_csv = pd.DataFrame(length_dict) \n",
    "# length_csv.to_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'length_comparison', data_date + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_to_int(ip): \n",
    "    return struct.unpack(\"!L\", socket.inet_aton(ip))[0] \n",
    "\n",
    "def tuple_to_vector(t):\n",
    "    # ip1_1, ip1_2, ip1_3, ip1_4, port1, ip2_1, ip2_2, ip2_3, ip2_4, port2, protocol = t\n",
    "    ip1_1, ip1_2, ip1_3, ip1_4, ip2_1, ip2_2, ip2_3, ip2_4 = t\n",
    "    return [\n",
    "        int(ip1_1), \n",
    "        int(ip1_2),\n",
    "        int(ip1_3),\n",
    "        int(ip1_4),\n",
    "        # int(port1),\n",
    "        int(ip2_1),\n",
    "        int(ip2_2),\n",
    "        int(ip2_3),\n",
    "        int(ip2_4),\n",
    "        # int(port2),\n",
    "        # int(protocol)\n",
    "    ] \n",
    "\n",
    "def tuple_to_vector2(t):\n",
    "    # ip1_1, ip1_2, ip1_3, ip1_4, port1, ip2_1, ip2_2, ip2_3, ip2_4, port2, protocol = t\n",
    "    ip1_1, ip1_2, ip1_3, ip1_4 = t\n",
    "    return [\n",
    "        int(ip1_1), \n",
    "        int(ip1_2),\n",
    "        int(ip1_3),\n",
    "        int(ip1_4),\n",
    "    ]\n",
    "\n",
    "def split_ip(ip_address):\n",
    "    return [int(octet) for octet in ip_address.split('.')] \n",
    "\n",
    "def ip_distance(ip1: list, ip2: list): \n",
    "    distance = abs(ip1 - ip2) \n",
    "    return abs(ip1 - ip2) \n",
    "\n",
    "def calculate_mean(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get5Tuples(conn_info: str): \n",
    "#     similarities = []\n",
    "#     if conn_info.startswith(conn_prefix + data_date): \n",
    "#         df_conn = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_info)) \n",
    "#         before_list = df_conn.apply(lambda row: \n",
    "#                                     split_ip(row['inRemoteIP']) + [row['inRemotePort']] + split_ip(row['inLocIP']) + [row['inLocPort'], 1], axis=1).to_list() \n",
    "#         after_list = df_conn.apply(lambda row: \n",
    "#                                     split_ip(row['outLocIP']) + [row['outLocPort']] + split_ip(row['outRemoteIP']) + [row['outRemotePort'], 1], axis=1).to_list() \n",
    "#         for before, after in zip(before_list, after_list): \n",
    "#             vec1 = tuple_to_vector(before) \n",
    "#             vec2 = tuple_to_vector(after) \n",
    "#             sim = cosine_similarity([vec1], [vec2])[0][0] \n",
    "#             # sim = 1 - cosine(vec1, vec2)\n",
    "#             similarities.append(sim) \n",
    "#     return similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get2Tuples(conn_info: str): # 拆分点分十进制来评估IP的余弦相似度\n",
    "    similarities = []\n",
    "    if conn_info.startswith(conn_prefix + data_date): \n",
    "        df_conn = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_info)) \n",
    "        before_list = df_conn.apply(lambda row: \n",
    "                                    split_ip(row['inRemoteIP']) + split_ip(row['inLocIP']), axis=1).to_list() \n",
    "        after_list = df_conn.apply(lambda row: \n",
    "                                    split_ip(row['outLocIP']) + split_ip(row['outRemoteIP']), axis=1).to_list() \n",
    "        for before, after in zip(before_list, after_list): \n",
    "            vec1 = tuple_to_vector(before) \n",
    "            vec2 = tuple_to_vector(after) \n",
    "            # sim = cosine_similarity([vec1], [vec2])[0][0] \n",
    "            sim = distance.euclidean(vec1, vec2)\n",
    "            # sim = 1 - cosine(vec1, vec2)\n",
    "            similarities.append(sim) \n",
    "    return similarities \n",
    "\n",
    "def getSrcIPDistance(conn_info: str): # 拆分点分十进制来评估IP的余弦相似度\n",
    "    similarities = []\n",
    "    if conn_info.startswith(conn_prefix + data_date): \n",
    "        df_conn = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_info)) \n",
    "        before_list = df_conn.apply(lambda row: \n",
    "                                    split_ip(row['inRemoteIP']), axis=1).to_list() \n",
    "        after_list = df_conn.apply(lambda row: \n",
    "                                    split_ip(row['outLocIP']), axis=1).to_list() \n",
    "        for before, after in zip(before_list, after_list): \n",
    "            vec1 = tuple_to_vector2(before) \n",
    "            vec2 = tuple_to_vector2(after) \n",
    "            # sim = cosine_similarity([vec1], [vec2])[0][0] \n",
    "            sim = distance.euclidean(vec1, vec2)\n",
    "            # sim = 1 - cosine(vec1, vec2)\n",
    "            similarities.append(sim) \n",
    "    return similarities \n",
    "\n",
    "def getDstIPDistance(conn_info: str): # 拆分点分十进制来评估IP的余弦相似度\n",
    "    similarities = []\n",
    "    if conn_info.startswith(conn_prefix + data_date): \n",
    "        df_conn = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_info)) \n",
    "        before_list = df_conn.apply(lambda row: \n",
    "                                    split_ip(row['inLocIP']), axis=1).to_list() \n",
    "        after_list = df_conn.apply(lambda row: \n",
    "                                    split_ip(row['outRemoteIP']), axis=1).to_list() \n",
    "        for before, after in zip(before_list, after_list): \n",
    "            vec1 = tuple_to_vector2(before) \n",
    "            vec2 = tuple_to_vector2(after) \n",
    "            # sim = cosine_similarity([vec1], [vec2])[0][0] \n",
    "            sim = distance.euclidean(vec1, vec2)\n",
    "            # sim = 1 - cosine(vec1, vec2)\n",
    "            similarities.append(sim) \n",
    "    return similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dict = {\n",
    "    'Name': [],\n",
    "    'Similarity': [],\n",
    "    'AverageSimilarity': []\n",
    "} \n",
    "def getSimilarity(): \n",
    "    for conn_info in os.listdir(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni')): \n",
    "        similarity = get2Tuples(conn_info) \n",
    "        similarity_dict['Name'].append(conn_info[12:-4]) \n",
    "        similarity_dict['Similarity'].append(similarity) \n",
    "        similarity_dict['AverageSimilarity'].append(calculate_mean(similarity))\n",
    "        similarity_csv = pd.DataFrame(similarity_dict) \n",
    "        similarity_csv.to_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_similarity', 'cosine_similarity', data_date + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_scalar(x, y):\n",
    "    return abs(x - y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-11-07--10-19-16 137009.38961392912 0.23093349225847198\n",
      "24-11-07--10-25-29 96041.93084294569 0.274250625667509\n",
      "24-11-07--10-32-20 39368.68434169867 42.677226690504106\n",
      "24-11-07--11-08-34 15527.794982540598 0.8833970023308895\n",
      "24-11-07--11-24-21 46523.03768274104 0.4980615513410874\n",
      "24-11-07--11-31-59 28786.79411009544 0.8600966288105891\n",
      "24-11-07--15-15-47 17060.18452255804 0.48002175737062713\n",
      "24-11-07--15-36-37 8191.719005192092 0.3838886091300865\n",
      "24-11-07--15-38-38 13741.716608357787 0.5107438963605363\n",
      "24-11-07--15-41-25 18377.732590874028 0.723788348675093\n",
      "24-11-07--15-42-58 39250.51078289472 0.6041286729035806\n",
      "24-11-07--15-46-14 37418.23418782454 0.5557590251342105\n",
      "24-11-07--15-52-05 15914.922088333922 0.520802869620752\n",
      "24-11-07--15-58-43 69763.78678492561 0.2930839440694509\n",
      "24-11-07--16-04-47 57374.03800105484 0.3881426094903079\n",
      "24-11-07--16-55-07 8234.414731413744 0.32381105333181975\n",
      "24-11-07--17-01-16 894.8757445672137 0.19819208759841647\n",
      "24-11-07--17-06-23 7594.087931057845 0.33756300879364814\n"
     ]
    }
   ],
   "source": [
    "M_prefix = 'M ' \n",
    "W_prefix = 'W ' \n",
    "features = ['Packets', 'Bytes', 'Rel Start', 'Duration', 'Flows'] \n",
    "\n",
    "distance_dict = {\n",
    "    'Name': [], \n",
    "    # 'Address A': [], \n",
    "    # 'Port A': [],\n",
    "    # 'Address B': [], \n",
    "    # 'Port B': [], \n",
    "    'Packets': [], \n",
    "    'Packets Rate': [],\n",
    "    'Bytes': [], \n",
    "    'Bytes Rate': [],\n",
    "    'Rel Start': [], \n",
    "    'Rel Start Rate': [],\n",
    "    'Duration': [], \n",
    "    'Duration Rate': [], \n",
    "    'Flows': [], \n",
    "    'Flows Rate': [], \n",
    "    'Bandwidth': [], \n",
    "    'Bandwith Rate': [] \n",
    "}\n",
    "\n",
    "# for conn_info in os.listdir(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni')): \n",
    "#     M_f_dict = {} \n",
    "#     W_f_dict = {} \n",
    "#     if conn_info.startswith(conn_prefix + data_date): \n",
    "#         df_conn = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_info)) \n",
    "#         distance_dict['Name'].append(data_date)\n",
    "#         for feature in features: # 初始化特征字典\n",
    "#             M_f = M_prefix + feature \n",
    "#             W_f = W_prefix + feature \n",
    "#             M_f_dict[M_f] = df_conn[M_f] \n",
    "#             W_f_dict[W_f] = df_conn[W_f] \n",
    "#             distance = 0\n",
    "#             distance_rate = 0\n",
    "#             for i in range(df_conn.shape[0]): \n",
    "#                 distance_rate += euclidean_distance_scalar(M_f_dict[M_f][i], W_f_dict[W_f][i]) / M_f_dict[M_f][i] \n",
    "#                 distance += euclidean_distance_scalar(M_f_dict[M_f][i], W_f_dict[W_f][i]) \n",
    "#             aver_distance = distance / df_conn.shape[0] \n",
    "#             aver_rate = distance_rate / df_conn.shape[0] \n",
    "#             distance_dict[feature].append(aver_distance) \n",
    "#             distance_dict[feature + ' Rate'].append(aver_rate) \n",
    "\n",
    "for conn_info in os.listdir(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni')): \n",
    "    if conn_info.startswith(conn_prefix + data_date): \n",
    "        df_conn = pd.read_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'conn_sni', conn_info)) \n",
    "        distance_dict['Name'].append(conn_info[12:-4])\n",
    "        \n",
    "        # IP地址\n",
    "        # src_ip_distance = getSrcIPDistance(conn_info) \n",
    "\n",
    "        M_T = M_prefix + 'Duration' \n",
    "        W_T = W_prefix + 'Duration' \n",
    "        M_Bytes = M_prefix + 'Bytes' \n",
    "        W_Bytes = W_prefix + 'Bytes' \n",
    "        non_zero_mask_M = df_conn[M_T] != 0 \n",
    "        non_zero_mask_W = df_conn[W_T] != 0 \n",
    "        M_bandwidth = df_conn[M_Bytes] / df_conn[M_T][non_zero_mask_M] \n",
    "        W_bandwidth = df_conn[W_Bytes] / df_conn[W_T][non_zero_mask_W] \n",
    "        average_bandwidth = (M_bandwidth - W_bandwidth).abs().mean() \n",
    "        average_bandwidth_rate  = ((M_bandwidth - W_bandwidth).abs() / M_bandwidth).mean() \n",
    "        distance_dict['Bandwidth'].append(round(average_bandwidth, 4)) \n",
    "        distance_dict['Bandwith Rate'].append(round(average_bandwidth_rate, 4))\n",
    "        print(conn_info[12:-4], average_bandwidth, average_bandwidth_rate) \n",
    "\n",
    "        for feature in features: # 其他特征\n",
    "            M_f = M_prefix + feature \n",
    "            W_f = W_prefix + feature \n",
    "            \n",
    "            # 计算绝对距离\n",
    "            distance = (df_conn[M_f] - df_conn[W_f]).abs().mean() \n",
    "            \n",
    "            # 计算相对变化率，避免除零\n",
    "            non_zero_mask = df_conn[M_f] != 0\n",
    "            distance_rate = ((df_conn[M_f][non_zero_mask] - df_conn[W_f][non_zero_mask]).abs() / df_conn[M_f][non_zero_mask]).mean()\n",
    "            \n",
    "            distance_dict[feature].append(round(distance, 4))\n",
    "            distance_dict[feature + ' Rate'].append(round(distance_rate, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_csv = pd.DataFrame(distance_dict) \n",
    "distance_csv.to_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_similarity', 'distance', data_date + '_improved.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# M_bandwidth and W_bandwidth data\n",
    "M_bandwidth = [\n",
    "    8495.579972084035, 32221.253749456282, 262.9211543267581, 6422.117026185943, \n",
    "    25225.642212450708, 62.818386203591864, 75.16341672337579, 367756.55562177, \n",
    "    284.3320057461646, 281.97256627384604, 140.96004648149903, 142.82397872955653, \n",
    "    334.28470092190815, 144.56722198218486, 141.42682978969415, 88.29053359294443, \n",
    "    284.4787525001988, 286.15870663274006, 109.92803885641197, 142.84101090261063, \n",
    "    141.67351078703774, 146.19965348009612, 837989.44153014\n",
    "]\n",
    "\n",
    "W_bandwidth = [\n",
    "    8615.318404759457, 16776.915221674655, 238.85423431293208, 3610.5337520168378, \n",
    "    120510.9743858279, 110.91967964762415, 109.50728024511643, 168826.8999510458, \n",
    "    15011.117133419715, 15184.17705363903, 11289.896595646514, 10132.429868309957, \n",
    "    294.39836037668397, 10292.259694018014, 10154.702068566543, 95.11148851389069, \n",
    "    22068.942943309296, 19724.98123547115, 15323.872313480095, 13542.954719425335, \n",
    "    14620.950511404119, 14870.895005651859, 415223.3859424236\n",
    "]\n",
    "\n",
    "# Convert to pandas Series\n",
    "M_series = pd.Series(M_bandwidth)\n",
    "W_series = pd.Series(W_bandwidth)\n",
    "\n",
    "# Calculate average_bandwidth and average_bandwidth_rate\n",
    "average_bandwidth = (M_series - W_series).abs().mean()\n",
    "average_bandwidth_rate = ((M_series - W_series).abs() / M_series).mean()\n",
    "\n",
    "average_bandwidth, average_bandwidth_rate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
