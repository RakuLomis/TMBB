{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tshark_directory = os.path.join('.', 'traffictracer')\n",
    "statistic_directory = ['WLAN_statistics', 'Meta_statistics'] \n",
    "ori_directory = ['WLAN', 'Meta'] \n",
    "port_directory = 'tshark_port' \n",
    "conn_directory = 'conn_in_out' \n",
    "evaluation_directory = 'evaluation'\n",
    "data_date = '24-11-07'\n",
    "\n",
    "meta_prefix = 'Meta-' \n",
    "wlan_prefix = 'WLAN-' \n",
    "conn_prefix = 'conn-in-out-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "direcrory_meta_statistic = os.path.join('.', 'wireshark_traffic', 'tshark', 'Meta_statistics') \n",
    "direcrory_meta = os.path.join('.', 'wireshark_traffic', 'tshark', 'Meta') \n",
    "direcrory_conn_in_out = os.path.join('.', 'wireshark_traffic', 'tshark', 'conn_in_out') \n",
    "\n",
    "conn_in_out_path = os.path.join(direcrory_conn_in_out, 'conn-in-out-24-09-26--17-03-48.csv') \n",
    "meta_statistic_path = os.path.join(direcrory_meta_statistic, 'Meta-24-09-26--16-59-01.csv') \n",
    "meta_path = os.path.join(direcrory_meta, 'Meta-24-09-26--16-59-01.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sni_dict = {\n",
    "    'Name': [], \n",
    "    'AllSNI': [], \n",
    "    'FilteredSNI': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conn_info in os.listdir(os.path.join(tshark_directory, conn_directory)): \n",
    "    if conn_info.startswith(conn_prefix + data_date): \n",
    "        df_conn = pd.read_csv(os.path.join(tshark_directory, conn_directory, conn_info)) \n",
    "        sni_unique = set(df_conn['Server Name'].dropna().unique()) \n",
    "        sni_dict['Name'].append(conn_info[12:-4]) \n",
    "        sni_dict['AllSNI'].append(sni_unique) \n",
    "        sni_dict['FilteredSNI'].append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sni_csv = pd.DataFrame(sni_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sni_csv.to_csv(os.path.join(tshark_directory, evaluation_directory, 'tt_stream_distribution', 'sni', data_date + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_file in os.listdir(os.path.join(tshark_directory, direcrory_meta)): \n",
    "    if meta_file.startswith('Meta' + data_date): \n",
    "        packets = pd.read_csv(os.path.join(tshark_directory, direcrory_meta, 'Meta' + data_date + '.csv')) # 防止读入同名pcap \n",
    "        packets = packets.dropna(subset=['TCP Stream index']) \n",
    "        packets_statistic = pd.read_csv(os.path.join(tshark_directory, direcrory_meta_statistic, 'Meta' + data_date + '.csv')) \n",
    "        packets_conn = pd.read_csv(os.path.join(tshark_directory, direcrory_conn_in_out, 'conn-in-out-' + data_date + '.csv')) \n",
    "        packets_server_name_unique = packets['Server Name'].dropna().unique() \n",
    "        domain_stream_all = {key: [] for key in packets_server_name_unique} \n",
    "        for name in domain_stream_all.keys(): # get the dictionary of sni: stream_ids\n",
    "            stream_ids = packets.loc[packets['Server Name'] == name, 'TCP Stream index'] \n",
    "            domain_stream_all[name].extend(stream_ids) \n",
    "        stream_domain_all = {stream_id: domain for domain, stream_ids in domain_stream_all.items() for stream_id in stream_ids} # reverse the dictionary\n",
    "        packets['Server Name'] = packets['TCP Stream index'].map(stream_domain_all) \n",
    "        packets_statistic['Server Name'] = packets_statistic['Stream ID'].map(stream_domain_all) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "packets = pd.read_csv(meta_path) # with tcp segment column\n",
    "packets = packets.dropna(subset=['TCP Stream index']) \n",
    "\n",
    "packets_statistic = pd.read_csv(meta_statistic_path) \n",
    "\n",
    "packets_conn = pd.read_csv(conn_in_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "packets_server_name_unique = packets['Server Name'].dropna().unique() \n",
    "domain_stream_all = {key: [] for key in packets_server_name_unique} \n",
    "for name in domain_stream_all.keys(): # get the dictionary of sni: stream_ids\n",
    "    stream_ids = packets.loc[packets['Server Name'] == name, 'TCP Stream index'] \n",
    "    domain_stream_all[name].extend(stream_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_domain_all = {stream_id: domain for domain, stream_ids in domain_stream_all.items() for stream_id in stream_ids} # reverse the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "packets['Server Name'] = packets['TCP Stream index'].map(stream_domain_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "packets_statistic['Server Name'] = packets_statistic['Stream ID'].map(stream_domain_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "packets_conn['Server Name'] = packets_conn['M Stream ID'].map(stream_domain_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rr3---sn-q4flrnld.googlevideo.com', 'ai.immersivetranslate.com', 'basic-static-server.infinitynewtab.com', 'www.gstatic.com', 'rr5---sn-a5meknzk.googlevideo.com', 'lh3.googleusercontent.com', 'api.infinitynewtab.com', 'beacons4.gvt2.com', 'clientservices.googleapis.com', 'accounts.youtube.com', 'beacons3.gvt2.com', 'rr5---sn-a5mekn6z.googlevideo.com', 'jnn-pa.googleapis.com', 'beacons2.gvt2.com', 'infinityicon.infinitynewtab.com', 'content-autofill.googleapis.com', 'clients2.google.com', 'play.google.com', 'android.clients.google.com', 'clients4.google.com', 'safebrowsing.googleapis.com', 'update.googleapis.com', 'www.google.com', 'infinity-permanent.infinitynewtab.com', 'rr3---sn-a5mekndz.googlevideo.com', 'www.youtube.com', 'memex-pa.googleapis.com', 'youtube.com', 'weatheroffer.com', 'rr1---sn-a5mlrnls.googlevideo.com', 'optimizationguide-pa.googleapis.com', 'www.googleapis.com', 'i.ytimg.com', 'beacons.gcp.gvt2.com', 'rr1---sn-q4fl6nsk.googlevideo.com', 'yt3.ggpht.com', 'accounts.google.com', 'infinitypro-img.infinitynewtab.com', 'mtalk.google.com', 'config.immersivetranslate.com', 'fonts.googleapis.com', 'fonts.gstatic.com', 'beacons.gvt2.com'}\n"
     ]
    }
   ],
   "source": [
    "print(set(packets_conn['Server Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
